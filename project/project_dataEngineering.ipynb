{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_dataEngineering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to1YX14hA5NK"
      },
      "source": [
        "#Data Engineering Notebook\n",
        "\n",
        "The report for this final project can be found at this [link](https://cybertraining-dsc.github.io/report/fa20-523-301/project/project/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neKgmN6QBFJa"
      },
      "source": [
        "## Part 1 Importing the functions\n",
        "\n",
        "This file requires that we import Numpy, Matplotlib, Pylab, Keras, and Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU7SDylhA3Qe",
        "outputId": "50a6256e-f374-4136-9681-ed24c1469a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install utils\n",
        "import numpy as np\n",
        "import pylab\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import warnings\n",
        "import utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9h_pp2vBV55"
      },
      "source": [
        "Now that the funtions have been imported the team can focus on the download coding. The following cells will set up an install for Kaggle files and prompt for an upload of the kaggle.json file for credentials. \n",
        "\n",
        "The mkdir function creates a directory for the Kaggle data. This cell will allow the team to verify that the kaggle.json file appropriately uploaded to the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTVOSTQjBYiN",
        "outputId": "63a300d2-8d4e-43e4-adc1-7a9f4ca87e07",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "##import the kaggle.json from local to drive\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "##when it asks you to choose a file select the kaggle.json located within the 'project' folder from the github repo\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-52b74414-1012-4395-8ea6-c926990dd95d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-52b74414-1012-4395-8ea6-c926990dd95d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"chelseagorius\",\"key\":\"0a34819ed937ff55d31f4288ab40cf19\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJZE3PjtBeQR",
        "outputId": "c159fb73-b0d8-4559-98e7-d5770a16826b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##make a kaggle and a data folder\n",
        "!mkdir ~/.kaggle\n",
        "!mkdir data\n",
        "##copy the kaggle.json to the .kaggle folder then grant permissions\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "#test to see if kaggle is working, should print list of datasets\n",
        "!kaggle datasets list"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "ref                                                       title                                         size  lastUpdated          downloadCount  \n",
            "--------------------------------------------------------  -------------------------------------------  -----  -------------------  -------------  \n",
            "manchunhui/us-election-2020-tweets                        US Election 2020 Tweets                      326MB  2020-11-08 12:37:15            894  \n",
            "unanimad/us-election-2020                                 US Election 2020                             418KB  2020-11-08 13:51:30            917  \n",
            "headsortails/us-election-2020-presidential-debates        US Election 2020 - Presidential Debates      199MB  2020-10-23 16:56:10            257  \n",
            "radustoicescu/2020-united-states-presidential-election    2020 United States presidential election      11MB  2019-07-04 15:00:45            578  \n",
            "etsc9287/2020-general-election-polls                      2020 General Election Polls                  109KB  2020-02-09 08:20:59            381  \n",
            "shivamb/netflix-shows                                     Netflix Movies and TV Shows                  971KB  2020-01-20 07:33:56          55577  \n",
            "terenceshin/covid19s-impact-on-airport-traffic            COVID-19's Impact on Airport Traffic         106KB  2020-10-19 12:40:17           3064  \n",
            "sootersaalu/amazon-top-50-bestselling-books-2009-2019     Amazon Top 50 Bestselling Books 2009 - 2019   15KB  2020-10-13 09:39:21           2979  \n",
            "nehaprabhavalkar/indian-food-101                          Indian Food 101                                7KB  2020-09-30 06:23:43           6513  \n",
            "omarhanyy/500-greatest-songs-of-all-time                  500 Greatest Songs of All Time                33KB  2020-10-26 13:36:09            926  \n",
            "heeraldedhia/groceries-dataset                            Groceries dataset                            257KB  2020-09-17 04:36:08           6916  \n",
            "andrewmvd/trip-advisor-hotel-reviews                      Trip Advisor Hotel Reviews                     5MB  2020-09-30 08:31:20           4668  \n",
            "karangadiya/fifa19                                        FIFA 19 complete player dataset                2MB  2018-12-21 03:52:59         102772  \n",
            "docstein/brics-world-bank-indicators                      BRICS World Bank Indicators                    4MB  2020-10-22 12:18:40            781  \n",
            "thomaskonstantin/highly-rated-children-books-and-stories  Highly Rated Children Books And Stories      106KB  2020-10-24 12:09:59            542  \n",
            "christianlillelund/donald-trumps-rallies                  Donald Trump's Rallies                       720KB  2020-09-26 10:25:08           1574  \n",
            "datasnaek/youtube-new                                     Trending YouTube Video Statistics            201MB  2019-06-03 00:56:47         113952  \n",
            "google/tinyquickdraw                                      QuickDraw Sketches                            11GB  2018-04-18 19:38:04           2426  \n",
            "anikannal/solar-power-generation-data                     Solar Power Generation Data                    2MB  2020-08-18 15:52:03           9203  \n",
            "zynicide/wine-reviews                                     Wine Reviews                                  51MB  2017-11-27 17:08:04         118079  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn6gsxIJBzoe"
      },
      "source": [
        "Now, the team must download all of the datasets for the class. The three datasets are focused on the NBA. \n",
        "\n",
        "The first dataset is for injuries. Each injury will be used to set up players, timeframes, and severity of injuries. \n",
        "\n",
        "The other two datasets are for the player performance. By cross referencing this data to the previous list, the team will be able to see which players are limited from the injury and how performance is hampered by time in rehab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rmkpN1YByux",
        "outputId": "c9fa130c-6f5c-4ad1-ac96-cba44b8d7857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##downloading all the datasets\n",
        "!kaggle datasets download -d ghopkins/nba-injuries-2010-2018\n",
        "!kaggle datasets download -d nathanlauga/nba-games\n",
        "!kaggle datasets download -d pablote/nba-enhanced-stats"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nba-injuries-2010-2018.zip to /content\n",
            "\r  0% 0.00/226k [00:00<?, ?B/s]\n",
            "100% 226k/226k [00:00<00:00, 83.0MB/s]\n",
            "Downloading nba-games.zip to /content\n",
            " 50% 9.00M/18.1M [00:00<00:00, 25.2MB/s]\n",
            "100% 18.1M/18.1M [00:00<00:00, 45.8MB/s]\n",
            "Downloading nba-enhanced-stats.zip to /content\n",
            " 54% 9.00M/16.7M [00:01<00:00, 10.4MB/s]\n",
            "100% 16.7M/16.7M [00:01<00:00, 16.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qopGnNNOB84J",
        "outputId": "1b058698-b5f4-44b6-9679-c200b2d8dc37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##unzipping to the data folder\n",
        "!unzip nba-injuries-2010-2018.zip -d data\n",
        "!unzip nba-games.zip -d data\n",
        "!unzip nba-enhanced-stats.zip -d data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  nba-injuries-2010-2018.zip\n",
            "  inflating: data/injuries_2010-2020.csv  \n",
            "Archive:  nba-games.zip\n",
            "  inflating: data/games.csv          \n",
            "  inflating: data/games_details.csv  \n",
            "  inflating: data/players.csv        \n",
            "  inflating: data/ranking.csv        \n",
            "  inflating: data/teams.csv          \n",
            "Archive:  nba-enhanced-stats.zip\n",
            "  inflating: data/2012-18_officialBoxScore.csv  \n",
            "  inflating: data/2012-18_playerBoxScore.csv  \n",
            "  inflating: data/2012-18_standings.csv  \n",
            "  inflating: data/2012-18_teamBoxScore.csv  \n",
            "  inflating: data/2016-17_officialBoxScore.csv  \n",
            "  inflating: data/2016-17_playerBoxScore.csv  \n",
            "  inflating: data/2016-17_standings.csv  \n",
            "  inflating: data/2016-17_teamBoxScore.csv  \n",
            "  inflating: data/2017-18_officialBoxScore.csv  \n",
            "  inflating: data/2017-18_playerBoxScore.csv  \n",
            "  inflating: data/2017-18_standings.csv  \n",
            "  inflating: data/2017-18_teamBoxScore.csv  \n",
            "  inflating: data/metadata_officialBoxScore.pdf  \n",
            "  inflating: data/metadata_playerBoxScore.pdf  \n",
            "  inflating: data/metadata_standing.pdf  \n",
            "  inflating: data/metadata_teamBoxScore.pdf  \n",
            "  inflating: data/teamBoxScore.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VyMM4w4CB6A"
      },
      "source": [
        "The team must now use these downloads to create dataframes. Pandas dataframes will be easier to manage the data. The team will be able to use Pandas to process the data and allow the team to make correlations for feature engineering to create the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNthqUMxB_N5"
      },
      "source": [
        "#create a list for each data set\n",
        "ds_NBA_Injuries, ds_NBA_Games, ds_NBA_Enhanced = [], [], []\n",
        "\n",
        "#import csv files as dataframes and save to respective list, injury set first\n",
        "df_Injuries = pd.read_csv('data/injuries_2010-2020.csv')\n",
        "df_Injury_Start = df_Injuries[df_Injuries.Acquired.isnull()]\n",
        "df_Injury_End = df_Injuries[df_Injuries.Relinquished.isnull()]\n",
        "ds_NBA_Injuries = [df_Injury_Start, df_Injury_End]\n",
        "#nba games dataset\n",
        "df_Games_games = pd.read_csv('data/games.csv')\n",
        "df_Games_gamesDetails = pd.read_csv('data/games_details.csv')\n",
        "df_Games_players = pd.read_csv('data/players.csv')\n",
        "df_Games_ranking = pd.read_csv('data/ranking.csv')\n",
        "df_Games_teams = pd.read_csv('data/teams.csv')\n",
        "ds_NBA_Games = [df_Games_games, df_Games_gamesDetails, df_Games_players, df_Games_ranking, df_Games_teams]\n",
        "#nba enhanced stats dataset\n",
        "df_En_officialBS_1218 = pd.read_csv('data/2012-18_officialBoxScore.csv')\n",
        "df_En_playerBS_1218 = pd.read_csv('data/2012-18_playerBoxScore.csv')\n",
        "df_En_standings_1218 = pd.read_csv('data/2012-18_standings.csv')\n",
        "df_En_teamBS_1218 = pd.read_csv('data/2012-18_teamBoxScore.csv')  \n",
        "df_En_officialBS_1617 = pd.read_csv('data/2016-17_officialBoxScore.csv')  \n",
        "df_En_playerBS_1617 = pd.read_csv('data/2016-17_playerBoxScore.csv')\n",
        "df_En_standings_1617 = pd.read_csv('data/2016-17_standings.csv')\n",
        "df_En_teamBS_1617 = pd.read_csv('data/2016-17_teamBoxScore.csv')  \n",
        "df_En_officialBS_1718 = pd.read_csv('data/2017-18_officialBoxScore.csv')  \n",
        "df_En_playerBS_1718 = pd.read_csv('data/2017-18_playerBoxScore.csv')\n",
        "df_En_standings_1718 = pd.read_csv('data/2017-18_standings.csv')\n",
        "df_En_teamBS_1718 = pd.read_csv('data/2017-18_teamBoxScore.csv')  \n",
        "##data/metadata_officialBoxScore.pdf, data/metadata_playerBoxScore.pdf, data/metadata_standing.pdf, data/metadata_teamBoxScore.pdf  \n",
        "df_En_teamBS = pd.read_csv('data/teamBoxScore.csv')\n",
        "ds_NBA_Enhanced = [df_En_officialBS_1218, df_En_officialBS_1617, df_En_officialBS_1718, df_En_playerBS_1218, df_En_playerBS_1617, df_En_playerBS_1718, df_En_standings_1218, df_En_standings_1617, df_En_standings_1718, \\\n",
        "                       df_En_teamBS_1218, df_En_teamBS_1617, df_En_teamBS_1718, df_En_teamBS]\n",
        "\n",
        "\n",
        "#probably need some more data exploration and some feature engineering"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEmzUVhYCWoR"
      },
      "source": [
        "Preparing data tables to have the appropriate columns in order to calculate time and player specific metrics for each injury."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQQ2M60LCRc5"
      },
      "source": [
        "#Create a dataframe of distinct player and player IDs\n",
        "df_distinct_playerID = df_Games_players[[\"PLAYER_NAME\", \"PLAYER_ID\"]].drop_duplicates()\n",
        "df_distinct_playerID.astype({'PLAYER_ID':'object'}).dtypes\n",
        "#Create a dataframe of distinct gameID and game dates\n",
        "df_Games_games['GAME_DATE_EST'] = pd.to_datetime(df_Games_games['GAME_DATE_EST'])\n",
        "df_distinct_gameId_date = df_Games_games[[\"GAME_ID\", \"GAME_DATE_EST\"]].drop_duplicates()\n",
        "#Join the distinct player df by player name and join the gameID information\n",
        "df_Injury_Start = df_Injury_Start.join(df_distinct_playerID.astype('object').set_index('PLAYER_NAME'), on='Relinquished')\n",
        "df_Injury_Start = df_Injury_Start.merge(df_Games_teams[[\"TEAM_ID\", \"NICKNAME\"]], left_on=\"Team\", right_on=\"NICKNAME\")\n",
        "df_Injury_Start.drop(['NICKNAME'], axis=1)#.apply(lambda x: x.date())\n",
        "df_Injury_Start['Date']= pd.to_datetime(df_Injury_Start['Date'])\n",
        "#Do the same for the Injury End database\n",
        "df_Injury_End = df_Injury_End.join(df_distinct_playerID.astype('object').set_index('PLAYER_NAME'), on='Acquired')\n",
        "df_Injury_End = df_Injury_End.merge(df_Games_teams[[\"TEAM_ID\", \"NICKNAME\"]], left_on=\"Team\", right_on=\"NICKNAME\")\n",
        "df_Injury_End.drop(['NICKNAME'], axis=1)#.apply(lambda x: x.date())\n",
        "df_Injury_End['Date']= pd.to_datetime(df_Injury_End['Date'])\n",
        "# df_distinct_playerID=df_distinct_playerID.sort_values('PLAYER_NAME')\n",
        "df_Games_gamesDetails = df_Games_gamesDetails.merge(df_distinct_gameId_date, on=\"GAME_ID\")\n",
        "#graph looking at difference in stats and injury length\n",
        "##calculating injury length could be hard\n",
        "df_Injury_length = df_Injury_Start.copy()\n",
        "df_Injury_length = df_Injury_length.rename(columns={\"Date\":\"DateInjured\", \"Relinquished\":\"Player\", \"Notes\":\"InjuryNotes\"})\n",
        "df_Injury_length = df_Injury_length[[\"TEAM_ID\", \"Team\", \"PLAYER_ID\", \"Player\", \"DateInjured\", \"InjuryNotes\"]]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWBgkuoLCzsn"
      },
      "source": [
        "Transforming the minutes column to a numeric value that can be used to create calculated metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlNyxrIDCqlA"
      },
      "source": [
        "for index, row in df_Games_gamesDetails.iterrows():\n",
        "  try:\n",
        "    m, s = str(row.MIN).split(':')\n",
        "  except (SyntaxError, ValueError) as e:\n",
        "    m = (row.MIN)\n",
        "    s = 0\n",
        "  df_Games_gamesDetails.loc[index,'MIN'] = pd.to_numeric(m) + pd.to_numeric(s)/60\n",
        "df_Games_gamesDetails.to_csv('df_Games_gamesDetails.csv')"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPFMNYvMfq_g"
      },
      "source": [
        "#df_Games_gamesDetails = pd.read_csv('df_Games_gamesDetails.csv')\n",
        "#df_Games_gamesDetails['GAME_DATE_EST'] = pd.to_datetime(df_Games_gamesDetails['GAME_DATE_EST'])"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlDThd6PXjKz"
      },
      "source": [
        "Creating db that contains the injury start and end date for each injury listed in the original db."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnTbGFVaRlif"
      },
      "source": [
        "df_Injury_length = df_Injury_length[df_Injury_length['Player'] != np.nan]\n",
        "inj_count = df_Injury_length['Player'].value_counts()\n",
        "#First find the date recovered\n",
        "for index, row in df_Injury_length.iterrows():\n",
        "  #get rows with same player ID\n",
        "  temp = df_Injury_End.loc[df_Injury_End['PLAYER_ID'] == row.PLAYER_ID]\n",
        "  #get rows after the injury date\n",
        "  temp2 = temp.loc[(temp['Date'] > row.DateInjured)]\n",
        "  #get the row with the oldest (smallest) date\n",
        "  recover = temp2.nsmallest(1, 'Date')\n",
        "  try:\n",
        "    df_Injury_length.at[index, 'DateRecovered'] = pd.Series(recover[['Date']].Date).values[0]\n",
        "    df_Injury_length.at[index, 'RecoverNotes'] = pd.Series(recover[['Notes']].Notes).values[0]\n",
        "  except (IndexError) as e:\n",
        "    df_Injury_length.at[index, 'DateRecovered'] = np.nan\n",
        "    df_Injury_length.at[index, 'RecoverNotes'] = np.nan\n",
        "  #Get number of injuries\n",
        "  count_name = row['Player']\n",
        "  try:\n",
        "   df_Injury_length.at[index,'NumberInjuries'] = inj_count[count_name]\n",
        "  except (KeyError) as e:\n",
        "   pass#print(inj_count[count_name])\n",
        "#Group by player Id and date recovered to avoid miscount of injuries\n",
        "group = df_Injury_length.groupby(['PLAYER_ID','DateRecovered'])['DateInjured'].min().reset_index()\n",
        "df_Injury_length = pd.merge(group, df_Injury_length,  how='left', left_on=['PLAYER_ID','DateRecovered', 'DateInjured'], right_on = ['PLAYER_ID','DateRecovered', 'DateInjured'])\n",
        "#Calculating injury length in days\n",
        "df_Injury_length['InjuryLengthDays'] = df_Injury_length['DateRecovered'] - df_Injury_length['DateInjured']\n",
        "for index, row in df_Injury_length.iterrows():\n",
        "  df_Injury_length.at[index, 'InjuryLengthDays'] = (row.InjuryLengthDays)\n",
        "#Saving to a .csv file\n",
        "df_Injury_length.to_csv('df_Injury_length.csv')"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgI5tzu5DX_g"
      },
      "source": [
        "Creating the metrics for player performance metrics during the injury game and summarized for the 5 games prior to the injury.\n",
        "Creating the metrics for player performance metrics in the first game back from injury and summarized for the 5 games after thedf_Injury_stats.to_csv('df_Injury_stat.csv')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lfiek9Poz8Y"
      },
      "source": [
        "df_Injury_stats = df_Injury_length.copy()\n",
        "for index, row in df_Injury_stats.iterrows():\n",
        "  #games of just that player\n",
        "  plyr = df_Games_gamesDetails.loc[df_Games_gamesDetails['PLAYER_ID'] == row.PLAYER_ID]\n",
        "  #games before and inlucding injury date#5 games prior and the game of injury, for some reason we need to have 4 different variabels, did not work with resetting the variable 'game_set' to itself\n",
        "  temp = plyr.loc[(plyr['GAME_DATE_EST'] <= row.DateInjured)]\n",
        "  inj_gameset = temp.nlargest(6, 'GAME_DATE_EST')\n",
        "  #games after and inlucding recover date\n",
        "  temp2 = plyr.loc[(plyr['GAME_DATE_EST'] >= row.DateRecovered)]\n",
        "  #5 games after and the game of injury, for some reason we need to have 3 different variabels, did not work with resetting the variable 'game_set' to itself\n",
        "  rev_gameset = temp.nsmallest(6, 'GAME_DATE_EST')\n",
        "  ##\n",
        "  ##Start calculated columns for injury games and prior\n",
        "  if len(inj_gameset) > 0:\n",
        "    #injury game\n",
        "    inj_game = inj_gameset.iloc[0]\n",
        "    #5 games prior to injury\n",
        "    prior5 = inj_gameset.iloc[1:]\n",
        "    #storing game data from injury game\n",
        "    df_Injury_stats.at[index, 'inj_MIN'] = inj_game[['MIN']].MIN\n",
        "    df_Injury_stats.at[index,'inj_FGA'] = inj_game[['FGA']].FGA\n",
        "    df_Injury_stats.at[index,'inj_FG_PCT'] = inj_game[['FG_PCT']].FG_PCT\n",
        "    df_Injury_stats.at[index,'inj_FG3A'] = inj_game[['FG3A']].FG3A\n",
        "    df_Injury_stats.at[index,'inj_FG3_PCT'] = inj_game[['FG3_PCT']].FG3_PCT\n",
        "    df_Injury_stats.loc[index,'inj_FTA'] = inj_game[['FTA']].FTA\n",
        "    df_Injury_stats.loc[index,'inj_FT_PCT'] = inj_game[['FT_PCT']].FT_PCT\n",
        "    df_Injury_stats.loc[index,'inj_REB'] = inj_game[['REB']].REB\n",
        "    df_Injury_stats.loc[index,'inj_AST'] = inj_game[['AST']].AST\n",
        "    df_Injury_stats.loc[index,'inj_STL'] = inj_game[['STL']].STL\n",
        "    df_Injury_stats.loc[index,'inj_BLK'] = inj_game[['BLK']].BLK\n",
        "    df_Injury_stats.loc[index,'inj_TO'] = inj_game[['TO']].TO\n",
        "    df_Injury_stats.loc[index,'inj_PF'] = inj_game[['PF']].PF\n",
        "    df_Injury_stats.loc[index,'inj_PTS'] = inj_game[['PTS']].PTS\n",
        "    df_Injury_stats.loc[index,'inj_PLUS_MINUS'] = inj_game[['PLUS_MINUS']].PLUS_MINUS\n",
        "#storing game data from prior 5 games\n",
        "    df_Injury_stats.at[index,'p5_MIN'] = prior5[['MIN']].MIN.mean()\n",
        "    df_Injury_stats.at[index,'p5_FGA'] = prior5[['FGA']].FGA.mean()\n",
        "    df_Injury_stats.at[index,'p5_FG_PCT'] = prior5[['FG_PCT']].FG_PCT.mean()\n",
        "    df_Injury_stats.at[index,'p5_FG3A'] = prior5[['FG3A']].FG3A.mean()\n",
        "    df_Injury_stats.at[index,'p5_FG3_PCT'] = prior5[['FG3_PCT']].FG3_PCT.mean()\n",
        "    df_Injury_stats.at[index,'p5_FTA'] = prior5[['FTA']].FTA.mean()\n",
        "    df_Injury_stats.at[index,'p5_FT_PCT'] = prior5[['FT_PCT']].FT_PCT.mean()\n",
        "    df_Injury_stats.at[index,'p5_REB'] = prior5[['REB']].REB.mean()\n",
        "    df_Injury_stats.at[index,'p5_AST'] = prior5[['AST']].AST.mean()\n",
        "    df_Injury_stats.at[index,'p5_STL'] = prior5[['STL']].STL.mean()\n",
        "    df_Injury_stats.at[index,'p5_BLK'] = prior5[['BLK']].BLK.mean()\n",
        "    df_Injury_stats.at[index,'p5_TO'] = prior5[['TO']].TO.mean()\n",
        "    df_Injury_stats.at[index,'p5_PF'] = prior5[['PF']].PF.mean()\n",
        "    df_Injury_stats.at[index,'p5_PTS'] = prior5[['PTS']].PTS.mean()\n",
        "    df_Injury_stats.at[index,'p5_PLUS_MINUS'] = prior5[['PLUS_MINUS']].PLUS_MINUS.mean()\n",
        "  ##\n",
        "  ##Star calculated column for recovery game and after\n",
        "  if len(rev_gameset) > 0:\n",
        "    #injury game\n",
        "    rev_game = rev_gameset.iloc[0]\n",
        "    #5 games post injury\n",
        "    post5 = rev_gameset.iloc[1:]\n",
        "    #storing game data from injury game\n",
        "    df_Injury_stats.at[index, 'inj_MIN'] = rev_game[['MIN']].MIN\n",
        "    df_Injury_stats.at[index,'inj_FGA'] = rev_game[['FGA']].FGA\n",
        "    df_Injury_stats.at[index,'inj_FG_PCT'] = rev_game[['FG_PCT']].FG_PCT\n",
        "    df_Injury_stats.at[index,'inj_FG3A'] = rev_game[['FG3A']].FG3A\n",
        "    df_Injury_stats.at[index,'inj_FG3_PCT'] = rev_game[['FG3_PCT']].FG3_PCT\n",
        "    df_Injury_stats.loc[index,'inj_FTA'] = rev_game[['FTA']].FTA\n",
        "    df_Injury_stats.loc[index,'inj_FT_PCT'] = rev_game[['FT_PCT']].FT_PCT\n",
        "    df_Injury_stats.loc[index,'inj_REB'] = rev_game[['REB']].REB\n",
        "    df_Injury_stats.loc[index,'inj_AST'] = rev_game[['AST']].AST\n",
        "    df_Injury_stats.loc[index,'inj_STL'] = rev_game[['STL']].STL\n",
        "    df_Injury_stats.loc[index,'inj_BLK'] = rev_game[['BLK']].BLK\n",
        "    df_Injury_stats.loc[index,'inj_TO'] = rev_game[['TO']].TO\n",
        "    df_Injury_stats.loc[index,'inj_PF'] = rev_game[['PF']].PF\n",
        "    df_Injury_stats.loc[index,'inj_PTS'] = rev_game[['PTS']].PTS\n",
        "    df_Injury_stats.loc[index,'inj_PLUS_MINUS'] = rev_game[['PLUS_MINUS']].PLUS_MINUS\n",
        "    #storing game data from prior 5 games\n",
        "    df_Injury_stats.at[index,'p5_MIN'] = post5[['MIN']].MIN.mean()\n",
        "    df_Injury_stats.at[index,'p5_FGA'] = post5[['FGA']].FGA.mean()\n",
        "    df_Injury_stats.at[index,'p5_FG_PCT'] = post5[['FG_PCT']].FG_PCT.mean()\n",
        "    df_Injury_stats.at[index,'p5_FG3A'] = post5[['FG3A']].FG3A.mean()\n",
        "    df_Injury_stats.at[index,'p5_FG3_PCT'] = post5[['FG3_PCT']].FG3_PCT.mean()\n",
        "    df_Injury_stats.at[index,'p5_FTA'] = post5[['FTA']].FTA.mean()\n",
        "    df_Injury_stats.at[index,'p5_FT_PCT'] = post5[['FT_PCT']].FT_PCT.mean()\n",
        "    df_Injury_stats.at[index,'p5_REB'] = post5[['REB']].REB.mean()\n",
        "    df_Injury_stats.at[index,'p5_AST'] = post5[['AST']].AST.mean()\n",
        "    df_Injury_stats.at[index,'p5_STL'] = post5[['STL']].STL.mean()\n",
        "    df_Injury_stats.at[index,'p5_BLK'] = post5[['BLK']].BLK.mean()\n",
        "    df_Injury_stats.at[index,'p5_TO'] = post5[['TO']].TO.mean()\n",
        "    df_Injury_stats.at[index,'p5_PF'] = post5[['PF']].PF.mean()\n",
        "    df_Injury_stats.at[index,'p5_PTS'] = post5[['PTS']].PTS.mean()\n",
        "    df_Injury_stats.at[index,'p5_PLUS_MINUS'] = post5[['PLUS_MINUS']].PLUS_MINUS.mean()\n",
        "        #print(inj_game)\n",
        "df_Injury_stats.to_csv('df_Injury_stat.csv')\n"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h18Qa9AVYCTx"
      },
      "source": [
        "#TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf_lXB24Y3g5"
      },
      "source": [
        ""
      ],
      "execution_count": 145,
      "outputs": []
    }
  ]
}