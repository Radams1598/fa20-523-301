{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_dataEngineering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to1YX14hA5NK"
      },
      "source": [
        "#Data Engineering Notebook\n",
        "\n",
        "The report for this final project can be found at this [link](https://cybertraining-dsc.github.io/report/fa20-523-301/project/project/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neKgmN6QBFJa"
      },
      "source": [
        "## Part 1 Importing the functions\n",
        "\n",
        "This file requires that we import Numpy, Matplotlib, Pylab, Keras, and Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU7SDylhA3Qe",
        "outputId": "5ac227da-3560-4186-f710-52e61554277c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import warnings\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9h_pp2vBV55"
      },
      "source": [
        "Now that the funtions have been imported the team can focus on the download coding. The following cells will set up an install for Kaggle files and prompt for an upload of the kaggle.json file for credentials. \n",
        "\n",
        "The mkdir function creates a directory for the Kaggle data. This cell will allow the team to verify that the kaggle.json file appropriately uploaded to the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTVOSTQjBYiN",
        "outputId": "b78c979e-82cb-4b91-82a3-ab89c2488753",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "##import the kaggle.json from local to drive\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "##when it asks you to choose a file select the kaggle.json located within the 'project' folder from the github repo\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4d0d3324-3154-4aa0-868b-28606ac84d7b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4d0d3324-3154-4aa0-868b-28606ac84d7b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"chelseagorius\",\"key\":\"0a34819ed937ff55d31f4288ab40cf19\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJZE3PjtBeQR",
        "outputId": "efe3cba9-3629-47a5-a5e1-36c1c07fcf41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##make a kaggle and a data folder\n",
        "!mkdir ~/.kaggle\n",
        "!mkdir data\n",
        "##copy the kaggle.json to the .kaggle folder then grant permissions\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "#test to see if kaggle is working, should print list of datasets\n",
        "!kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "ref                                                       title                                               size  lastUpdated          downloadCount  \n",
            "--------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  \n",
            "terenceshin/covid19s-impact-on-airport-traffic            COVID-19's Impact on Airport Traffic               106KB  2020-10-19 12:40:17           1290  \n",
            "sootersaalu/amazon-top-50-bestselling-books-2009-2019     Amazon Top 50 Bestselling Books 2009 - 2019         15KB  2020-10-13 09:39:21           1258  \n",
            "thomaskonstantin/highly-rated-children-books-and-stories  Highly Rated Children Books And Stories            106KB  2020-10-24 12:09:59            288  \n",
            "tunguz/euro-parliament-proceedings-1996-2011              Euro Parliament Proceedings 1996 - 2011              1GB  2020-10-26 17:48:29             18  \n",
            "rishidamarla/judicial-expenditures-across-all-50-states   Judicial Expenditures across all 50 States           2KB  2020-10-25 00:07:45            194  \n",
            "docstein/brics-world-bank-indicators                      BRICS World Bank Indicators                          4MB  2020-10-22 12:18:40            277  \n",
            "kanishk307/6000-indian-food-recipes-dataset               6000+ Indian Food Recipes Dataset                    9MB  2020-10-24 01:08:23            360  \n",
            "elvinagammed/chatbots-intent-recognition-dataset          Chatbots: Intent Recognition Dataset                17KB  2020-10-23 07:44:59            152  \n",
            "omarhanyy/500-greatest-songs-of-all-time                  500 Greatest Songs of All Time                      33KB  2020-10-26 13:36:09            323  \n",
            "balraj98/synthetic-objective-testing-set-sots-reside      Synthetic Objective Testing Set (SOTS) [RESIDE]    415MB  2020-10-24 10:07:29             48  \n",
            "lunamcbride24/pokemon-type-matchup-data                   Pokemon Type Matchup Data                            9KB  2020-10-14 18:56:23            198  \n",
            "gaurav2796/kaggle-competions-rankings-and-kernels         Kaggle Competions, Rankings and Kernels            698KB  2020-10-15 04:05:15             28  \n",
            "balraj98/indoor-training-set-its-residestandard           Indoor Training Set (ITS) [RESIDE-Standard]          5GB  2020-10-24 10:07:30             29  \n",
            "romazepa/moscow-schools-winners-of-educational-olympiads  Moscow schools - winners of educational Olympiads    1MB  2020-10-12 21:45:01             45  \n",
            "sootersaalu/nigerian-songs-spotify                        Nigerian Songs Spotify                              24KB  2020-10-25 19:10:23             48  \n",
            "salmaneunus/mechanical-tools-dataset                      Mechanical Tools Classification Dataset            652MB  2020-11-01 11:28:22            155  \n",
            "thanatoz/hinglish-blogs                                   Hinglish blogs                                       2MB  2020-10-13 18:16:05             12  \n",
            "shivamb/netflix-shows                                     Netflix Movies and TV Shows                        971KB  2020-01-20 07:33:56          52361  \n",
            "nehaprabhavalkar/indian-food-101                          Indian Food 101                                      7KB  2020-09-30 06:23:43           5821  \n",
            "heeraldedhia/groceries-dataset                            Groceries dataset                                  257KB  2020-09-17 04:36:08           6021  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn6gsxIJBzoe"
      },
      "source": [
        "Now, the team must download all of the datasets for the class. The three datasets are focused on the NBA. \n",
        "\n",
        "The first dataset is for injuries. Each injury will be used to set up players, timeframes, and severity of injuries. \n",
        "\n",
        "The other two datasets are for the player performance. By cross referencing this data to the previous list, the team will be able to see which players are limited from the injury and how performance is hampered by time in rehab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rmkpN1YByux",
        "outputId": "00fd16ac-a001-4aef-8b2b-f7679cc8b789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##downloading all the datasets\n",
        "!kaggle datasets download -d ghopkins/nba-injuries-2010-2018\n",
        "!kaggle datasets download -d nathanlauga/nba-games\n",
        "!kaggle datasets download -d pablote/nba-enhanced-stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nba-injuries-2010-2018.zip to /content\n",
            "\r  0% 0.00/226k [00:00<?, ?B/s]\n",
            "100% 226k/226k [00:00<00:00, 33.5MB/s]\n",
            "Downloading nba-games.zip to /content\n",
            " 50% 9.00M/18.1M [00:01<00:01, 5.38MB/s]\n",
            "100% 18.1M/18.1M [00:01<00:00, 10.4MB/s]\n",
            "Downloading nba-enhanced-stats.zip to /content\n",
            " 54% 9.00M/16.7M [00:00<00:00, 16.8MB/s]\n",
            "100% 16.7M/16.7M [00:00<00:00, 28.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qopGnNNOB84J",
        "outputId": "09028b4b-e1d4-4699-8502-0d6105c96cf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##unzipping to the data folder\n",
        "!unzip nba-injuries-2010-2018.zip -d data\n",
        "!unzip nba-games.zip -d data\n",
        "!unzip nba-enhanced-stats.zip -d data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  nba-injuries-2010-2018.zip\n",
            "  inflating: data/injuries_2010-2020.csv  \n",
            "Archive:  nba-games.zip\n",
            "  inflating: data/games.csv          \n",
            "  inflating: data/games_details.csv  \n",
            "  inflating: data/players.csv        \n",
            "  inflating: data/ranking.csv        \n",
            "  inflating: data/teams.csv          \n",
            "Archive:  nba-enhanced-stats.zip\n",
            "  inflating: data/2012-18_officialBoxScore.csv  \n",
            "  inflating: data/2012-18_playerBoxScore.csv  \n",
            "  inflating: data/2012-18_standings.csv  \n",
            "  inflating: data/2012-18_teamBoxScore.csv  \n",
            "  inflating: data/2016-17_officialBoxScore.csv  \n",
            "  inflating: data/2016-17_playerBoxScore.csv  \n",
            "  inflating: data/2016-17_standings.csv  \n",
            "  inflating: data/2016-17_teamBoxScore.csv  \n",
            "  inflating: data/2017-18_officialBoxScore.csv  \n",
            "  inflating: data/2017-18_playerBoxScore.csv  \n",
            "  inflating: data/2017-18_standings.csv  \n",
            "  inflating: data/2017-18_teamBoxScore.csv  \n",
            "  inflating: data/metadata_officialBoxScore.pdf  \n",
            "  inflating: data/metadata_playerBoxScore.pdf  \n",
            "  inflating: data/metadata_standing.pdf  \n",
            "  inflating: data/metadata_teamBoxScore.pdf  \n",
            "  inflating: data/teamBoxScore.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VyMM4w4CB6A"
      },
      "source": [
        "The team must now use these downloads to create dataframes. Pandas dataframes will be easier to manage the data. The team will be able to use Pandas to process the data and allow the team to make correlations for feature engineering to create the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNthqUMxB_N5"
      },
      "source": [
        "#create a list for each data set\n",
        "ds_NBA_Injuries, ds_NBA_Games, ds_NBA_Enhanced = [], [], []\n",
        "\n",
        "#import csv files as dataframes and save to respective list, injury set first\n",
        "df_Injuries = pd.read_csv('data/injuries_2010-2020.csv')\n",
        "df_Injury_Start = df_Injuries[df_Injuries.Acquired.isnull()]\n",
        "df_Injury_End = df_Injuries[df_Injuries.Relinquished.isnull()]\n",
        "ds_NBA_Injuries = [df_Injury_Start, df_Injury_End]\n",
        "#nba games dataset\n",
        "df_Games_games = pd.read_csv('data/games.csv')\n",
        "df_Games_gamesDetails = pd.read_csv('data/games_details.csv')\n",
        "df_Games_players = pd.read_csv('data/players.csv')\n",
        "df_Games_ranking = pd.read_csv('data/ranking.csv')\n",
        "df_Games_teams = pd.read_csv('data/teams.csv')\n",
        "ds_NBA_Games = [df_Games_games, df_Games_gamesDetails, df_Games_players, df_Games_ranking, df_Games_teams]\n",
        "#nba enhanced stats dataset\n",
        "df_En_officialBS_1218 = pd.read_csv('data/2012-18_officialBoxScore.csv')\n",
        "df_En_playerBS_1218 = pd.read_csv('data/2012-18_playerBoxScore.csv')\n",
        "df_En_standings_1218 = pd.read_csv('data/2012-18_standings.csv')\n",
        "df_En_teamBS_1218 = pd.read_csv('data/2012-18_teamBoxScore.csv')  \n",
        "df_En_officialBS_1617 = pd.read_csv('data/2016-17_officialBoxScore.csv')  \n",
        "df_En_playerBS_1617 = pd.read_csv('data/2016-17_playerBoxScore.csv')\n",
        "df_En_standings_1617 = pd.read_csv('data/2016-17_standings.csv')\n",
        "df_En_teamBS_1617 = pd.read_csv('data/2016-17_teamBoxScore.csv')  \n",
        "df_En_officialBS_1718 = pd.read_csv('data/2017-18_officialBoxScore.csv')  \n",
        "df_En_playerBS_1718 = pd.read_csv('data/2017-18_playerBoxScore.csv')\n",
        "df_En_standings_1718 = pd.read_csv('data/2017-18_standings.csv')\n",
        "df_En_teamBS_1718 = pd.read_csv('data/2017-18_teamBoxScore.csv')  \n",
        "##data/metadata_officialBoxScore.pdf, data/metadata_playerBoxScore.pdf, data/metadata_standing.pdf, data/metadata_teamBoxScore.pdf  \n",
        "df_En_teamBS = pd.read_csv('data/teamBoxScore.csv')\n",
        "ds_NBA_Enhanced = [df_En_officialBS_1218, df_En_officialBS_1617, df_En_officialBS_1718, df_En_playerBS_1218, df_En_playerBS_1617, df_En_playerBS_1718, df_En_standings_1218, df_En_standings_1617, df_En_standings_1718, \\\n",
        "                       df_En_teamBS_1218, df_En_teamBS_1617, df_En_teamBS_1718, df_En_teamBS]\n",
        "\n",
        "\n",
        "#probably need some more data exploration and some feature engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEmzUVhYCWoR"
      },
      "source": [
        "Preparing data tables to have the appropriate columns in order to calculate time and player specific metrics for each injury."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQQ2M60LCRc5"
      },
      "source": [
        "#distinct player and player ID list\n",
        "df_distinct_playerID = df_Games_players[[\"PLAYER_NAME\", \"PLAYER_ID\"]].drop_duplicates()\n",
        "df_distinct_playerID.astype({'PLAYER_ID':'object'}).dtypes\n",
        "#distinct gameID and game date list\n",
        "df_Games_games['GAME_DATE_EST'] = pd.to_datetime(df_Games_games['GAME_DATE_EST'])\n",
        "df_distinct_gameId_date = df_Games_games[[\"GAME_ID\", \"GAME_DATE_EST\"]].drop_duplicates()\n",
        "#join player ID, for j=injury start db\n",
        "df_Injury_Start = df_Injury_Start.join(df_distinct_playerID.astype('object').set_index('PLAYER_NAME'), on='Relinquished')\n",
        "df_Injury_Start = df_Injury_Start.merge(df_Games_teams[[\"TEAM_ID\", \"NICKNAME\"]], left_on=\"Team\", right_on=\"NICKNAME\")\n",
        "df_Injury_Start.drop(['NICKNAME'], axis=1)\n",
        "df_Injury_Start['Date']= pd.to_datetime(df_Injury_Start['Date'])\n",
        "#again for injury end db\n",
        "df_Injury_End = df_Injury_End.join(df_distinct_playerID.astype('object').set_index('PLAYER_NAME'), on='Acquired')\n",
        "df_Injury_End = df_Injury_End.merge(df_Games_teams[[\"TEAM_ID\", \"NICKNAME\"]], left_on=\"Team\", right_on=\"NICKNAME\")\n",
        "df_Injury_End.drop(['NICKNAME'], axis=1)\n",
        "df_Injury_End['Date']= pd.to_datetime(df_Injury_Start['Date'])\n",
        "# df_distinct_playerID=df_distinct_playerID.sort_values('PLAYER_NAME')\n",
        "df_Games_gamesDetails = df_Games_gamesDetails.merge(df_distinct_gameId_date, on=\"GAME_ID\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRbSj8NdEQhQ"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWBgkuoLCzsn"
      },
      "source": [
        "Transforming the minutes column to a numeric value that can be used to create calculated metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlNyxrIDCqlA"
      },
      "source": [
        "for index, row in df_Games_gamesDetails.iterrows():\n",
        "  try:\n",
        "    m, s = str(row.MIN).split(':')\n",
        "  except (SyntaxError, ValueError) as e:\n",
        "    m = (row.MIN)\n",
        "    s = 0\n",
        "  df_Games_gamesDetails.loc[index,'MIN'] = pd.to_numeric(m) + pd.to_numeric(s)/60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyKvKFohDQQ3"
      },
      "source": [
        "Creating the metrics for player performance metrics during the injury game and summarized for the 5 games prior to the injury."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mG54V94DHrf"
      },
      "source": [
        "for index, row in df_Injury_Start.iterrows():\n",
        "        #games of just that player\n",
        "        temp = df_Games_gamesDetails.loc[df_Games_gamesDetails['PLAYER_ID'] == row.PLAYER_ID]\n",
        "        #games before and inlucding injury date\n",
        "        inj_game = temp.loc[(temp['GAME_DATE_EST'] == row.Date)]\n",
        "        #5 games prior and the game of injury, for some reason we need to have 4 different variabels, did not work with resetting the variable 'game_set' to itself\n",
        "        temp2 = temp.loc[(temp['GAME_DATE_EST'] <= row.Date)]\n",
        "        game_set = temp2.nlargest(6, 'GAME_DATE_EST')\n",
        "        if len(game_set) > 0:\n",
        "          #injury game\n",
        "          inj_game = game_set.iloc[0]\n",
        "          #5 games prior to injury\n",
        "          prior5 = game_set.iloc[1:]\n",
        "          #storing game data from injury game\n",
        "          df_Injury_Start.at[index, 'inj_MIN'] = inj_game[['MIN']].MIN\n",
        "          df_Injury_Start.at[index,'inj_FGA'] = inj_game[['FGA']].FGA\n",
        "          df_Injury_Start.at[index,'inj_FG_PCT'] = inj_game[['FG_PCT']].FG_PCT\n",
        "          df_Injury_Start.at[index,'inj_FG3A'] = inj_game[['FG3A']].FG3A\n",
        "          df_Injury_Start.at[index,'inj_FG3_PCT'] = inj_game[['FG3_PCT']].FG3_PCT\n",
        "          df_Injury_Start.loc[index,'inj_FTA'] = inj_game[['FTA']].FTA\n",
        "          df_Injury_Start.loc[index,'inj_FT_PCT'] = inj_game[['FT_PCT']].FT_PCT\n",
        "          df_Injury_Start.loc[index,'inj_REB'] = inj_game[['REB']].REB\n",
        "          df_Injury_Start.loc[index,'inj_AST'] = inj_game[['AST']].AST\n",
        "          df_Injury_Start.loc[index,'inj_STL'] = inj_game[['STL']].STL\n",
        "          df_Injury_Start.loc[index,'inj_BLK'] = inj_game[['BLK']].BLK\n",
        "          df_Injury_Start.loc[index,'inj_TO'] = inj_game[['TO']].TO\n",
        "          df_Injury_Start.loc[index,'inj_PF'] = inj_game[['PF']].PF\n",
        "          df_Injury_Start.loc[index,'inj_PTS'] = inj_game[['PTS']].PTS\n",
        "          df_Injury_Start.loc[index,'inj_PLUS_MINUS'] = inj_game[['PLUS_MINUS']].PLUS_MINUS\n",
        "#storing game data from prior 5 games\n",
        "          df_Injury_Start.at[index,'p5_MIN'] = prior5[['MIN']].MIN.mean()\n",
        "          df_Injury_Start.at[index,'p5_FGA'] = prior5[['FGA']].FGA.mean()\n",
        "          df_Injury_Start.at[index,'p5_FG_PCT'] = prior5[['FG_PCT']].FG_PCT.mean()\n",
        "          df_Injury_Start.at[index,'p5_FG3A'] = prior5[['FG3A']].FG3A.mean()\n",
        "          df_Injury_Start.at[index,'p5_FG3_PCT'] = prior5[['FG3_PCT']].FG3_PCT.mean()\n",
        "          df_Injury_Start.at[index,'p5_FTA'] = prior5[['FTA']].FTA.mean()\n",
        "          df_Injury_Start.at[index,'p5_FT_PCT'] = prior5[['FT_PCT']].FT_PCT.mean()\n",
        "          df_Injury_Start.at[index,'p5_REB'] = prior5[['REB']].REB.mean()\n",
        "          df_Injury_Start.at[index,'p5_AST'] = prior5[['AST']].AST.mean()\n",
        "          df_Injury_Start.at[index,'p5_STL'] = prior5[['STL']].STL.mean()\n",
        "          df_Injury_Start.at[index,'p5_BLK'] = prior5[['BLK']].BLK.mean()\n",
        "          df_Injury_Start.at[index,'p5_TO'] = prior5[['TO']].TO.mean()\n",
        "          df_Injury_Start.at[index,'p5_PF'] = prior5[['PF']].PF.mean()\n",
        "          df_Injury_Start.at[index,'p5_PTS'] = prior5[['PTS']].PTS.mean()\n",
        "          df_Injury_Start.at[index,'p5_PLUS_MINUS'] = prior5[['PLUS_MINUS']].PLUS_MINUS.mean()\n",
        "          \n",
        "df_Injury_Start.to_csv('df_Injury_Start.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgI5tzu5DX_g"
      },
      "source": [
        "Creating the metrics for player performance metrics in the first game back from injury and summarized for the 5 games after teh return."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-vjKw-LDYLY"
      },
      "source": [
        "#df_Injury_End\n",
        "for index, row in df_Injury_End.iterrows():\n",
        "        #games of just that player\n",
        "        temp = df_Games_gamesDetails.loc[df_Games_gamesDetails['PLAYER_ID'] == row.PLAYER_ID]\n",
        "        #games before and inlucding injury date\n",
        "        temp2 = temp.loc[(temp['GAME_DATE_EST'] >= row.Date)]\n",
        "        #5 games prior and the game of injury, for some reason we need to have 4 different variabels, did not work with resetting the variable 'game_set' to itself\n",
        "        game_set = temp.nsmallest(6, 'GAME_DATE_EST')\n",
        "        if len(game_set) > 0:\n",
        "          #injury game\n",
        "          inj_game = game_set.iloc[0]\n",
        "          #5 games post injury\n",
        "          post5 = game_set.iloc[1:]\n",
        "          #storing game data from injury game\n",
        "          df_Injury_End.at[index, 'inj_MIN'] = inj_game[['MIN']].MIN\n",
        "          df_Injury_End.at[index,'inj_FGA'] = inj_game[['FGA']].FGA\n",
        "          df_Injury_End.at[index,'inj_FG_PCT'] = inj_game[['FG_PCT']].FG_PCT\n",
        "          df_Injury_End.at[index,'inj_FG3A'] = inj_game[['FG3A']].FG3A\n",
        "          df_Injury_End.at[index,'inj_FG3_PCT'] = inj_game[['FG3_PCT']].FG3_PCT\n",
        "          df_Injury_End.loc[index,'inj_FTA'] = inj_game[['FTA']].FTA\n",
        "          df_Injury_End.loc[index,'inj_FT_PCT'] = inj_game[['FT_PCT']].FT_PCT\n",
        "          df_Injury_End.loc[index,'inj_REB'] = inj_game[['REB']].REB\n",
        "          df_Injury_End.loc[index,'inj_AST'] = inj_game[['AST']].AST\n",
        "          df_Injury_End.loc[index,'inj_STL'] = inj_game[['STL']].STL\n",
        "          df_Injury_End.loc[index,'inj_BLK'] = inj_game[['BLK']].BLK\n",
        "          df_Injury_End.loc[index,'inj_TO'] = inj_game[['TO']].TO\n",
        "          df_Injury_End.loc[index,'inj_PF'] = inj_game[['PF']].PF\n",
        "          df_Injury_End.loc[index,'inj_PTS'] = inj_game[['PTS']].PTS\n",
        "          df_Injury_End.loc[index,'inj_PLUS_MINUS'] = inj_game[['PLUS_MINUS']].PLUS_MINUS\n",
        "          #storing game data from prior 5 games\n",
        "          df_Injury_End.at[index,'p5_MIN'] = post5[['MIN']].MIN.mean()\n",
        "          df_Injury_End.at[index,'p5_FGA'] = post5[['FGA']].FGA.mean()\n",
        "          df_Injury_End.at[index,'p5_FG_PCT'] = post5[['FG_PCT']].FG_PCT.mean()\n",
        "          df_Injury_End.at[index,'p5_FG3A'] = post5[['FG3A']].FG3A.mean()\n",
        "          df_Injury_End.at[index,'p5_FG3_PCT'] = post5[['FG3_PCT']].FG3_PCT.mean()\n",
        "          df_Injury_End.at[index,'p5_FTA'] = post5[['FTA']].FTA.mean()\n",
        "          df_Injury_End.at[index,'p5_FT_PCT'] = post5[['FT_PCT']].FT_PCT.mean()\n",
        "          df_Injury_End.at[index,'p5_REB'] = post5[['REB']].REB.mean()\n",
        "          df_Injury_End.at[index,'p5_AST'] = post5[['AST']].AST.mean()\n",
        "          df_Injury_End.at[index,'p5_STL'] = post5[['STL']].STL.mean()\n",
        "          df_Injury_End.at[index,'p5_BLK'] = post5[['BLK']].BLK.mean()\n",
        "          df_Injury_End.at[index,'p5_TO'] = post5[['TO']].TO.mean()\n",
        "          df_Injury_End.at[index,'p5_PF'] = post5[['PF']].PF.mean()\n",
        "          df_Injury_End.at[index,'p5_PTS'] = post5[['PTS']].PTS.mean()\n",
        "          df_Injury_End.at[index,'p5_PLUS_MINUS'] = post5[['PLUS_MINUS']].PLUS_MINUS.mean()\n",
        "        #print(inj_game)\n",
        "\n",
        "        #print(inj_game)\n",
        "        #print(prior5)\n",
        "df_Injury_End.to_csv('df_Injury_End.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}